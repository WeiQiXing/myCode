## 图卷积网络
---
主要知识： 基于不动点理论的图神经网络（Graph Neural Network，GNN），图卷积神经网络（Graph Convolutional Neural Network，GCN）

### 历史脉络
1. 图神经网络概念最早在2005年提出。2009年Franco博士在其论文中定义了图神经网络的理论基础。
2. 最早的GNN主要解决的还是如分子结构分类等严格意义上的图论问题。但实际上欧式空间（比如图像Image）或者是序列（比如想文本Text），许多常见场景也都可以转换为图（Graph），然后就可以通过图神经网络技术来建模了。
3. 2009年后图神经网络也陆续有一些研究，但是没有太大波澜。直到2013年，在图信号处理（Graph Signal Processing）的基础上，Bruna在文献中首次提出图上的基于频域（Spectral-domain）和基于空域（Spatial-domain）的卷积神经网络。
4. 其后，学界提出了很多基于空域的图卷积方式，而基于频域的工作相对较少，只受到部分学者的青睐。
5. 值得一提，图神经网络与图表示学习（Represent Learning for Graph）的发展历程也惊人相似。2014年，在word2vec的启发下，Perozzi等人提出了DeepWalk，开启了深度学习时代图表示学习的大门。更有趣的是，几乎就在同时，Bordes等人提出了大名鼎鼎的TransE，为知识图谱的分布式表示（Represent Learning for Knowledge Graph）奠定了基础。

### 图神经网络（Graph Neural Network）
这里提到的图均指图论中的图（Graph），这是一种由若干个结点（Node）及连接两个结点的边（Edge）所构成的图形，用于刻画不同结点之间的关系。

![image-20191203190452556](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20191203190452556.png)

#### 状态更新与输出
最早的图神经网络起源于Franco博士的论文，它的理论基础是不动点理论。给定一张图G，每个结点都有其自己的特征（feature），这里使用$$x_v $$表示结点v的特征；连接两个结点的边也有自己的特征，本文用$$x_(v,u) $$ 表示结点v和结点u之间边的特征；GNN学习目标是获得每个结点的图感知的隐藏状态$$h_v  (state embedding)$$, 这就意味着：对于每个结点，它的隐藏状态包含了来自邻居结点的信息。那么，如何让每个结点都感知到图上其他的结点呢？GNN通过迭代式更新所有结点的隐藏状态来实现，在t+1时刻，结点v的隐藏状态按照如下方式更新：
$$ 𝐡^{t+1}_𝑣=𝑓(𝐱_𝑣,𝐱_𝑐𝑜[𝑣],𝐡^{t}_𝑛𝑒[𝑣] ,𝐱_𝑛𝑒[𝑣]), $$
上面公式中的f就是隐藏状态的状态更新函数，在论文中也被称为局部转移函数（local transaction function）。公式中的$$x_c o[v]$$指的是与结点v相邻的边的特征，$$x_n e[v] $$指的是结点v的邻居结点的特征，$$h^t_n e[v] $$则指邻居结点在t时刻的隐藏状态。注意f是对所有结点都成立的，是一个全局共享的函数。那么怎么将其与深度学习结合在一起呢？就是利用神经网络（Neural Network）来拟合这个复杂函数f。值得一提的是，虽然看起来f的输入是不定长参数，但在f内部可以将不定长的参数通过一定操作编程一个固定的参数，比如说用所有隐藏状态的加和来代表所有隐藏状态。举栗子：

![image-20191203192030599](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20191203192030599.png)

假设结点5为中心结点，其隐藏状态的更新函数如图所示，这个更新公式表达的思想自然又贴切：不断地利用当前时刻邻居结点的隐藏状态作为部分输入来生成下一时刻中心结点的隐藏状态，直到每个结点的隐藏状态变化幅度很小，整个图的信息流动区域平稳。至此，每个结点都“知晓”了其邻居的信息。状态更新公式仅描述了如何获取每个结点的隐藏状态，除它之外，我们还需要另外一个函数g来描述如何适应下游任务。举栗子，给定一个社交网络，一个可能的下游任务是判断每个结点是否为水军账号。
$$𝐨_𝑣=𝑔(𝐡_𝑣,𝐱_𝑣)$$

在原论文中，g又被称为局部输出函数（local input function），与f类似，g也可以由一个神经网络来表达，它也是一个全局共享的函数，那么整个流程可以用下图来表示：

![image-20191203193042161](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20191203193042161.png)

仔细观察两个时刻之间的连线，它与图的连线密切相关。比如说在$$T_1$$时刻，结点1的状态接受来自结点3的上一时刻的隐藏状态，因为结点1与结点3相邻。直到$$T_n$$时刻，各个结点隐藏状态收敛，每个结点后面接一个g即可得到该结点的输出o。
对于不同的图来说，收敛的时刻可能不同，因为收敛是通过两个时刻p-范数的差值是否小于某个阈值$$\epsilon$$来判定的，比如：
$$||\mathbf{H}^{t+1}||{2}-||\mathbf{H}^{t}||{2}<\epsilon$$

### 不动点理论
GNN的理论基础是不动点(the fixed point)理论，这里的不动点理论专指巴拿赫不动点定理(Banach's Fixed Point Therem).首先我们用F表示若干个f堆叠得到的一个函数，也称为全局更新函数，那么图上所有结点的状态更新公式可以写成：
$$𝐇^{𝑡+1}=F(𝐇^𝑡,𝐗)$$
不动点定理指的就是，不论$\mathbf{H}^0$是什么，只要 $F$ 是个压缩映射(contraction map)，$\mathbf{H}^{0}$经过不断迭代都会收敛到某一个固定的点，我们称之为不动点。那压缩映射又是什么呢，一张图可以解释得明明白白：

![image-20191204181612574](C:\Users\Administrator\AppData\Roaming\Typora\typora-user-images\image-20191204181612574.png)

上图的实线箭头就是指映射$F$,任意两个点$x,y$在经过$F$这个映射后，分别变成了$F(x),F(y)$。压缩映射就是指，$𝑑(𝐹(𝑥),𝐹(𝑦))≤𝑐𝑑(𝑥,𝑦), 0≤𝑐<1$。也就是说，经过 $F$ 变换后的新空间一定比原先的空间要小，原先的空间被压缩了。想象这种压缩的过程不断进行，最终就会把原空间中的所有点映射到一个点上。

### 具体实现
在具体实现中， $f$ 其实通过一个简单的前馈神经网络(Feed-forward Neural Network)即可实现。比如说，一种实现方法可以是把每个邻居结点的特征、隐藏状态、每条相连边的特征以及结点本身的特征简单拼接在一起，在经过前馈神经网络后做一次简单的加和。

$$𝐡_𝑣^{𝑡+1}=𝑓(𝐱_𝑣,𝐱_𝑐𝑜[𝑣] ,𝐡^t_𝑛𝑒[𝑣] ,𝐱_𝑛𝑒[𝑣])$$ $$=\sum_{𝑢∈𝑛𝑒[𝑣]} FNN([𝐱_𝑣;𝐱_{(𝑢,𝑣)};𝐡_𝑢^𝑡;𝐱_𝑢])$$

那我们如何保证 $f$ 是个压缩映射呢，其实是通过限制 $f$ 对 $\mathbf{H}$ 的偏导数矩阵的大小，这是通过一个对雅可比矩阵(Jacobian Matrix)的惩罚项(Penalty)来实现的。在代数中，有一个定理是: $f$ 为压缩映射的等价条件是 $f$ 的梯度/导数要小于1。这个等价定理可以从压缩映射的形式化定义导出，我们这里使用 $||x||$ 表示 $x$ 在空间中的范数(norm)。范数是一个标量，它是向量的长度或者模，$||x||$ 是 $x$ 在有限空间中坐标的连续函数。这里把 $x$ 简化成1维的，坐标之间的差值可以看作向量在空间中的距离，根据压缩映射的定义，可以导出：

$$||F(x)-F(y)||{\leq}c||x-y||, 0\ {\leq}c<1$$ $$\frac{||F(x)-F(y)||}{||x-y||}{\leq}c$$ $$\frac{||F(x)-F(x-{\Delta}x)||}{||{\Delta}x||}{\leq}c$$ $$||F'(x)||=||\frac{{\partial}F(x)}{{\partial}x}||{\leq}c$$

推广一下，即得到雅可比矩阵的罚项需要满足其范数小于等于$c$等价于压缩映射的条件。根据拉格朗日乘子法，将有约束问题变成带罚项的无约束优化问题，训练的目标可表示成如下形式：

$$J = Loss + \lambda \cdot \max({\frac{||{\partial}FNN||}{||{\partial}\mathbf{h}||}}−c,0), c\in(0,1)$$

其中$\lambda$是超参数，与其相乘的项即为雅可比矩阵的罚项。

### 模型学习
上面我们花一定的篇幅搞懂了如何让 $f$ 接近压缩映射，下面我们来具体叙述一下图神经网络中的损失 $Loss$ 是如何定义，以及模型是如何学习的。

仍然以社交网络举例，虽然每个结点都会有隐藏状态以及输出，但并不是每个结点都会有监督信号(Supervision)。比如说，社交网络中只有部分用户被明确标记了是否为水军账号，这就构成了一个典型的结点二分类问题。

那么很自然地，模型的损失即通过这些有监督信号的结点得到。假设监督结点一共有 $p$ 个，模型损失可以形式化为：

$$L𝑜𝑠𝑠=∑_{𝑖=1}^𝑝{(𝐭_𝑖−𝐨_𝑖)}$$

那么，模型如何学习呢？根据前向传播计算损失的过程，不难推出反向传播计算梯度的过程。在前向传播中，模型：

1. 调用 $f$ 若干次，比如 $T_n$次，直到 $\mathbf{h}^{T_{n}}_v$ 收敛。
2. 此时每个结点的隐藏状态接近不动点的解。
3. 对于有监督信号的结点，将其隐藏状态通过 $g$ 得到输出，进而算出模型的损失。
根据上面的过程，在反向传播时，我们可以直接求出 $f$ 和 $g$ 对最终的隐藏状态 $\mathbf{h}^{T_{n}}_v$ 的梯度。然而，因为模型递归调用了 $f$ 若干次，为计算 $f$ 和 $g$ 对最初的隐藏状态 $\mathbf{h}_v^0$ 的梯度，我们需要同样递归式/迭代式地计算 $T_n$ 次梯度。最终得到的梯度即为 $f$ 和 $g$ 对 $\mathbf{h}_v^0$ 的梯度，然后该梯度用于更新模型的参数。这个算法就是 Almeida-Pineda 算法





### 图卷积







